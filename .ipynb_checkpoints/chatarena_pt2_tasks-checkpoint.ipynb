{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Overview** \n",
    "As the capabilities of Large Language Models (LLMs) continue to grow, evaluating their performance has become more important than ever, especially in the face of the biases that can be embedded in them [1]. Human evaluation of LLMs is crucial, yet poses a variety of challenges due to the inherently subjective nature of human assessments. What one user considers to be a “good” response may not be perceived as such by another user, leading to inconsistent feedback of LLM performance [2]. Additionally, there is currently no “standard” methodology or consensus on best practices for the human evaluation of LLMs, making it challenging to compare different research results studying their performance [3]. Ultimately, these factors make it difficult to integrate stable human evaluations into the development and refinement process of LLMs. \n",
    "\n",
    "\n",
    "In an effort to address this gap, a group of researchers created the “Chatbot Arena,” which is “an open platform for evaluating LLMs based on human preferences” [4]. Much like using a traditional LLM, users can pose questions (hereinafter referred to as “prompts”) and receive answers (hereinafter referred to as “responses”) from an LLM. However, unlike traditional LLMs, users receive two different responses from two different models presented side-by-side. The user is asked to select the response they prefer or select a tie. The model is anonymous, meaning that the user does not know which model produced the responses until after they have selected the winner. This eliminates any potential biases that might arise from name recognition or previous experiences with specific models.\n",
    "\n",
    "\n",
    "Using a select dataset of over 17,000 conversations from the Chatbot Arena, we attempt to investigate and understand user preferences in various LLM interactions. Based on the user’s prompt, the two model responses, the selected “winner,” topic modeling data, and the prompt’s hardness scores, we attempt to complete two primary tasks. The first task (Task A) is the prediction of the winning model between Model A and Model B. The second task (Task B) is the prediction of the hardness score of the user’s prompt. \n",
    "\n",
    "\n",
    "The research questions for this project are as follows:\n",
    "* **RQ1:** *Can we use logistic regression to predict if “Model A” or “Model B” will be selected as the winner by a random user in a Chatbot Arena “battle”?*\n",
    "* **RQ2:** *Can we use linear regression to predict the hardness score of a given prompt?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In this notebook, you will the find logistic and linear regression models that have been feature engineered to predict model performance in battle and prompt difficulty in response to RQ1 and RQ2. It is the second of two notebooks used for this project.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nuhZF6AxHc7"
   },
   "source": [
    "# **1. Import libraries & data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igTEjtbQw1AZ",
    "outputId": "ff9147e8-6d63-47c9-a0c3-ca69743f20aa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from collections import defaultdict  # https://docs.python.org/3/library/collections.html Return a new dictionary-like object.\n",
    "import json, math, gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import textstat\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGtoHeGepX9F",
    "outputId": "c8300420-537e-4295-f6b4-432bbd767a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully into Colab.\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "import requests\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "# URL of the raw .npy file\n",
    "file_url = \"https://github.com/dychenster/nlp-chatarena/blob/main/chatbot-arena-prompts-embeddings.npy?raw=true\"\n",
    "\n",
    "# Use requests to get the file content\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# Make sure the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Load the content into a numpy array\n",
    "    content = BytesIO(response.content)\n",
    "    embeddings = np.load(content)\n",
    "    print(\"Embeddings loaded successfully into Colab.\")\n",
    "else:\n",
    "    print(f\"Failed to load the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "abIJU_L4w-iF",
    "outputId": "b100d01a-e6cf-4352-cc8e-99548fc04740"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>judge</th>\n",
       "      <th>conversation_a</th>\n",
       "      <th>conversation_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>chatglm-6b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>model_b</td>\n",
       "      <td>arena_user_973</td>\n",
       "      <td>[{'content': 'What is the difference between O...</td>\n",
       "      <td>[{'content': 'What is the difference between O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id     model_a    model_b   winner  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d  chatglm-6b  koala-13b  model_b   \n",
       "\n",
       "            judge                                     conversation_a  \\\n",
       "0  arena_user_973  [{'content': 'What is the difference between O...   \n",
       "\n",
       "                                      conversation_b  \n",
       "0  [{'content': 'What is the difference between O...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Chatbot Arena Conversations Dataset\n",
    "\n",
    "# Provide the raw URL of the JSON file\n",
    "url_conversations = \"https://raw.githubusercontent.com/dychenster/nlp-chatarena/main/chatbot-arena-conversations.jsonl.gz\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "conversations = pd.read_json(url_conversations, lines=True)\n",
    "\n",
    "# Display the first row of the data\n",
    "conversations.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7YTglh4A_Pc",
    "outputId": "d7115d98-cdf9-4dfb-b59f-fca3091576d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25322, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Related Topic & Hardness Dataset\n",
    "\n",
    "# Provide the raw URL of the JSON file\n",
    "url_scores = \"https://raw.githubusercontent.com/dychenster/nlp-chatarena/main/chatbot-arena-gpt3-scores.jsonl.gz\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "topic_and_hardness = pd.read_json(url_scores, lines=True)\n",
    "\n",
    "# Display the first row of the data\n",
    "topic_and_hardness.head(1)\n",
    "topic_and_hardness.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_cvv7z9Xg4S"
   },
   "source": [
    "# **2. Data Cleaning & Preparing for Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6tXmmjWxWJ1"
   },
   "source": [
    "## 2a. ChatArena Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7lnAy-JUxeGv"
   },
   "outputs": [],
   "source": [
    "# Create a prompt column\n",
    "conversations[\"prompt\"] = conversations[\"conversation_a\"].str[0].str[\"content\"]\n",
    "\n",
    "# Create a response columns\n",
    "conversations[\"model_a_response\"] = conversations[\"conversation_a\"].str[1].str[\"content\"]\n",
    "conversations[\"model_b_response\"] = conversations[\"conversation_b\"].str[1].str[\"content\"]\n",
    "\n",
    "# Create prompt and response length columns\n",
    "conversations[\"prompt_length\"] = conversations[\"prompt\"].str.len()\n",
    "conversations[\"response_a_length\"] = conversations[\"model_a_response\"].str.len()\n",
    "conversations[\"response_b_length\"] = conversations[\"model_b_response\"].str.len()\n",
    "\n",
    "# Remove rows with tie (bothbad) and tie as winners\n",
    "conversations = conversations.loc[~conversations[\"winner\"].isin([\"tie (bothbad)\", \"tie\"])]\n",
    "\n",
    "# Remove conversation_a and conversation_b columns\n",
    "conversations.drop(columns=[\"conversation_a\", \"conversation_b\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "dC3SSL7nzHXk",
    "outputId": "07b51ae8-0ee1-46a8-cbf9-6dcd9a9cd650"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_name</th>\n",
       "      <th>judge</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a_response</th>\n",
       "      <th>model_b_response</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>response_a_length</th>\n",
       "      <th>response_b_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>chatglm-6b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>model_b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>arena_user_973</td>\n",
       "      <td>What is the difference between OpenCL and CUDA?</td>\n",
       "      <td>OpenCL and CUDA are two different programming ...</td>\n",
       "      <td>OpenCL and CUDA are both programming languages...</td>\n",
       "      <td>47</td>\n",
       "      <td>892</td>\n",
       "      <td>1905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id     model_a    model_b   winner  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d  chatglm-6b  koala-13b  model_b   \n",
       "\n",
       "  winner_name           judge  \\\n",
       "0   koala-13b  arena_user_973   \n",
       "\n",
       "                                            prompt  \\\n",
       "0  What is the difference between OpenCL and CUDA?   \n",
       "\n",
       "                                    model_a_response  \\\n",
       "0  OpenCL and CUDA are two different programming ...   \n",
       "\n",
       "                                    model_b_response  prompt_length  \\\n",
       "0  OpenCL and CUDA are both programming languages...             47   \n",
       "\n",
       "   response_a_length  response_b_length  \n",
       "0                892               1905  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorganize conversations dataframe columns\n",
    "\n",
    "conversations[\"winner_name\"] = np.where(conversations[\"winner\"] == \"model_a\", conversations[\"model_a\"], conversations[\"model_b\"])\n",
    "col = conversations.pop(\"winner_name\")\n",
    "conversations.insert(4, \"winner_name\", col)\n",
    "\n",
    "conversations.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRZN5CYP6RQs",
    "outputId": "42b74a0e-71ec-4146-9bf0-b27038c51de1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt-4                     0.85\n",
       "claude-v1                 0.80\n",
       "claude-instant-v1         0.76\n",
       "gpt-3.5-turbo             0.71\n",
       "vicuna-13b                0.62\n",
       "guanaco-33b               0.60\n",
       "palm-2                    0.59\n",
       "wizardlm-13b              0.54\n",
       "koala-13b                 0.51\n",
       "vicuna-7b                 0.45\n",
       "mpt-7b-chat               0.35\n",
       "alpaca-13b                0.35\n",
       "oasst-pythia-12b          0.35\n",
       "gpt4all-13b-snoozy        0.34\n",
       "RWKV-4-Raven-14B          0.34\n",
       "chatglm-6b                0.30\n",
       "fastchat-t5-3b            0.30\n",
       "stablelm-tuned-alpha-7b   0.26\n",
       "dolly-v2-12b              0.23\n",
       "llama-13b                 0.20\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate win rate of each model\n",
    "\n",
    "conversations[\"winner_name\"].value_counts()\n",
    "\n",
    "win_rates = conversations[\"winner_name\"].value_counts() / (conversations[\"model_a\"].value_counts() + conversations[\"model_b\"].value_counts())\n",
    "win_rates.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dBhNz9hYxyn0",
    "outputId": "6ab9daea-5f88-42e4-93a1-2f9b9faf87fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The proportion of times the longer response was chosen as the 'winner': 0.6331321260898726\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at how often the longer responses were selected as the \"winner\"\n",
    "\n",
    "conversations[\"length_difference\"] = conversations[\"response_a_length\"] - conversations[\"response_b_length\"]\n",
    "conversations[\"longer_response\"] = conversations.apply(lambda row: \"model_a\" if row[\"length_difference\"] > 0 else \"model_b\", axis=1)\n",
    "\n",
    "longer_response_chosen_count = (conversations['winner'] == conversations['longer_response']).sum()\n",
    "\n",
    "display(\"The proportion of times the longer response was chosen as the 'winner': \" + str(longer_response_chosen_count/len(conversations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA0iYrHg5KFn"
   },
   "source": [
    "## 2b. Topic & Hardness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjydUgbO5TPC",
    "outputId": "a615798d-abcd-462b-cdf5-55844a08530d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_value_1    0\n",
      "score_value_2    0\n",
      "score_value_3    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "question_id                         object\n",
       "prompt                              object\n",
       "openai_scores_raw_choices_nested    object\n",
       "topic_modeling_1                    object\n",
       "score_reason_1                      object\n",
       "score_value_1                        int64\n",
       "topic_modeling_2                    object\n",
       "score_reason_2                      object\n",
       "score_value_2                        int64\n",
       "topic_modeling_3                    object\n",
       "score_reason_3                      object\n",
       "score_value_3                        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove null values from scores values\n",
    "topic_and_hardness = topic_and_hardness.dropna(subset=['score_value_1', 'score_value_2', 'score_value_3'])\n",
    "print(topic_and_hardness[['score_value_1', 'score_value_2', 'score_value_3']].isna().sum())\n",
    "\n",
    "# Change all score values to be numeric\n",
    "topic_and_hardness.loc[:,'score_value_1'] = pd.to_numeric(topic_and_hardness['score_value_1'], errors='coerce')\n",
    "topic_and_hardness.loc[:,'score_value_2'] = pd.to_numeric(topic_and_hardness['score_value_2'], errors='coerce')\n",
    "topic_and_hardness.loc[:,'score_value_3'] = pd.to_numeric(topic_and_hardness['score_value_3'], errors='coerce')\n",
    "\n",
    "# Drop any resulting null values\n",
    "topic_and_hardness= topic_and_hardness.dropna(subset=['score_value_1', 'score_value_2', 'score_value_3'])\n",
    "\n",
    "# Change all score values to be integers\n",
    "topic_and_hardness['score_value_1'] = topic_and_hardness['score_value_1'].astype(np.int64)\n",
    "topic_and_hardness['score_value_2'] = topic_and_hardness['score_value_2'].astype(np.int64)\n",
    "topic_and_hardness['score_value_3'] = topic_and_hardness['score_value_3'].astype(np.int64)\n",
    "\n",
    "# Verify all score values are integers\n",
    "topic_and_hardness.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m38Qydp6o9w"
   },
   "source": [
    "## 2c. Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "EDfFPtGf5hV7",
    "outputId": "7570e129-a3e3-41a1-acd0-583be5e66603"
   },
   "outputs": [],
   "source": [
    "# Merge conversations and topic/hardness dataframes\n",
    "merged_df = conversations.merge(topic_and_hardness, on=\"question_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "iOuQmbvTA5mh",
    "outputId": "a8cbcb86-7f4d-43cd-c0a6-a4d05f66f007"
   },
   "outputs": [],
   "source": [
    "# Reorganize merged dataframe\n",
    "\n",
    "# Remove certain columns\n",
    "merged_df.drop(columns=[\"openai_scores_raw_choices_nested\", \"topic_modeling_1\", \"topic_modeling_2\", \"topic_modeling_3\", \"score_reason_1\", \"score_reason_2\", \"score_reason_3\", \"prompt_y\"], inplace=True)\n",
    "\n",
    "# Rename prompt column\n",
    "merged_df.rename(columns={\"prompt_x\":\"prompt\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71tOquRW7Dpv",
    "outputId": "82ef28bb-bc8c-4497-d6e2-468c971960dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_name</th>\n",
       "      <th>judge</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model_a_response</th>\n",
       "      <th>model_b_response</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>response_a_length</th>\n",
       "      <th>response_b_length</th>\n",
       "      <th>length_difference</th>\n",
       "      <th>longer_response</th>\n",
       "      <th>score_value_1</th>\n",
       "      <th>score_value_2</th>\n",
       "      <th>score_value_3</th>\n",
       "      <th>average_score</th>\n",
       "      <th>total_hardness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58210e39b3fd4441a2bd4a518bb44c2d</td>\n",
       "      <td>chatglm-6b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>model_b</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>arena_user_973</td>\n",
       "      <td>What is the difference between OpenCL and CUDA?</td>\n",
       "      <td>OpenCL and CUDA are two different programming ...</td>\n",
       "      <td>OpenCL and CUDA are both programming languages...</td>\n",
       "      <td>47</td>\n",
       "      <td>892</td>\n",
       "      <td>1905</td>\n",
       "      <td>-1013</td>\n",
       "      <td>model_b</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8.67</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id     model_a    model_b   winner  \\\n",
       "0  58210e39b3fd4441a2bd4a518bb44c2d  chatglm-6b  koala-13b  model_b   \n",
       "\n",
       "  winner_name           judge  \\\n",
       "0   koala-13b  arena_user_973   \n",
       "\n",
       "                                            prompt  \\\n",
       "0  What is the difference between OpenCL and CUDA?   \n",
       "\n",
       "                                    model_a_response  \\\n",
       "0  OpenCL and CUDA are two different programming ...   \n",
       "\n",
       "                                    model_b_response  prompt_length  \\\n",
       "0  OpenCL and CUDA are both programming languages...             47   \n",
       "\n",
       "   response_a_length  response_b_length  length_difference longer_response  \\\n",
       "0                892               1905              -1013         model_b   \n",
       "\n",
       "   score_value_1  score_value_2  score_value_3  average_score  total_hardness  \n",
       "0              9              8              9           8.67              26  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add total (average) score for each battle\n",
    "merged_df[\"average_score\"] = (merged_df[\"score_value_1\"] + merged_df[\"score_value_2\"] + merged_df[\"score_value_3\"])/ 3\n",
    "\n",
    "# Compute total hardness of each prompt\n",
    "merged_df[\"total_hardness\"] = merged_df[\"score_value_1\"] + merged_df[\"score_value_2\"] + merged_df[\"score_value_3\"]\n",
    "\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "MauQo_bu8GUv",
    "outputId": "f1542a59-b89e-4e6b-cf89-8c1b23e784bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winner_name\n",
       "claude-instant-v1         7.26\n",
       "gpt-4                     7.25\n",
       "gpt-3.5-turbo             7.22\n",
       "vicuna-7b                 7.21\n",
       "claude-v1                 7.20\n",
       "palm-2                    7.14\n",
       "fastchat-t5-3b            7.12\n",
       "wizardlm-13b              7.10\n",
       "oasst-pythia-12b          7.07\n",
       "vicuna-13b                7.07\n",
       "koala-13b                 7.06\n",
       "RWKV-4-Raven-14B          7.05\n",
       "stablelm-tuned-alpha-7b   7.05\n",
       "mpt-7b-chat               7.00\n",
       "guanaco-33b               6.97\n",
       "gpt4all-13b-snoozy        6.93\n",
       "dolly-v2-12b              6.91\n",
       "alpaca-13b                6.84\n",
       "chatglm-6b                6.81\n",
       "llama-13b                 6.67\n",
       "Name: average_score, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average hardness score per model (in winning rows)\n",
    "average_hardness_per_model = merged_df.groupby(\"winner_name\")[\"average_score\"].mean().sort_values(ascending=False)\n",
    "average_hardness_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17045, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W36rhUzpzu63"
   },
   "source": [
    "# **3. Task A** \n",
    "# Given a prompt, can we predict which model’s response will win the user vote?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the merged dataframe\n",
    "task_a_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "k6_6Xo0yG0LA"
   },
   "outputs": [],
   "source": [
    "# One-hot encode the model names\n",
    "\n",
    "# Create an array with all the model names\n",
    "models_combined = pd.concat([task_a_df[\"model_a\"], task_a_df[\"model_b\"]]).unique()\n",
    "\n",
    "# Initialize the columns to zero\n",
    "for model in models_combined:\n",
    "    task_a_df[model] = 0\n",
    "\n",
    "# Use logical OR to combine the one-hot encoding for both model_a and model_b\n",
    "for model in models_combined:\n",
    "    task_a_df[model] = ((task_a_df[\"model_a\"] == model) | (task_a_df[\"model_b\"] == model)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "M03ykkLMEDmo"
   },
   "outputs": [],
   "source": [
    "# Add a new feature for the win rate of model_a\n",
    "task_a_df['model_a_win_rate'] = task_a_df['model_a'].map(win_rates)\n",
    "\n",
    "# Add a new feature for the win rate of model_b\n",
    "task_a_df['model_b_win_rate'] = task_a_df['model_b'].map(win_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use texstat library for text statistics\n",
    "\n",
    "# Flesch Reading Ease\n",
    "task_a_df[\"model_a_complexity\"] = (task_a_df[\"model_a_response\"]).apply(textstat.flesch_reading_ease)\n",
    "task_a_df[\"model_b_complexity\"] = (task_a_df[\"model_b_response\"]).apply(textstat.flesch_reading_ease)\n",
    "task_a_df[\"prompt_complexity\"] = (task_a_df[\"prompt\"]).apply(textstat.flesch_reading_ease)\n",
    "\n",
    "# Readability consensus (grade level)\n",
    "task_a_df[\"model_a_grade\"] = task_a_df[\"model_a_response\"].apply(lambda x: textstat.text_standard(x, float_output=True))\n",
    "task_a_df[\"model_b_grade\"] = task_a_df[\"model_b_response\"].apply(lambda x: textstat.text_standard(x, float_output=True))\n",
    "task_a_df[\"prompt_grade\"] = task_a_df[\"prompt\"].apply(lambda x: textstat.text_standard(x, float_output=True))\n",
    "\n",
    "# Reading time\n",
    "task_a_df[\"model_a_time\"] = task_a_df[\"model_a_response\"].apply(lambda x: textstat.reading_time(x, ms_per_char=14.69))\n",
    "task_a_df[\"model_b_time\"] = task_a_df[\"model_b_response\"].apply(lambda x: textstat.reading_time(x, ms_per_char=14.69))\n",
    "task_a_df[\"prompt_time\"] = task_a_df[\"prompt\"].apply(lambda x: textstat.reading_time(x, ms_per_char=14.69))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URHHcAFFczsY"
   },
   "source": [
    "## 3b. Modelling - Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3vAH9zt36MN",
    "outputId": "f2f45ffe-f1de-49df-f42a-ff3dce61bc9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.750073335288941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     model_a       0.73      0.77      0.75      1665\n",
      "     model_b       0.77      0.73      0.75      1744\n",
      "\n",
      "    accuracy                           0.75      3409\n",
      "   macro avg       0.75      0.75      0.75      3409\n",
      "weighted avg       0.75      0.75      0.75      3409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Features for model\n",
    "features = [\"prompt_length\", \"prompt_complexity\", \"prompt_grade\", \"prompt_time\", \"model_a_win_rate\", \"model_b_win_rate\",\n",
    "            'chatglm-6b', 'koala-13b', 'vicuna-13b', 'stablelm-tuned-alpha-7b','oasst-pythia-12b', 'dolly-v2-12b', 'alpaca-13b', 'llama-13b', 'fastchat-t5-3b', 'gpt-3.5-turbo', \\\n",
    "            'gpt-4', 'claude-v1', 'RWKV-4-Raven-14B', 'mpt-7b-chat', 'palm-2', 'claude-instant-v1', 'vicuna-7b', 'wizardlm-13b', 'gpt4all-13b-snoozy', 'guanaco-33b']\n",
    "\n",
    "\n",
    "X = task_a_df[features]\n",
    "y = task_a_df[\"winner\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model_task_a = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Train the model\n",
    "model_task_a.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred_encoded = model_task_a.predict(X_test)\n",
    "\n",
    "# Decode the predictions back to 'model_a' and 'model_b'\n",
    "y_pred = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Calculate the accuracy, precision, recall\n",
    "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred_encoded, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "id": "ztHlziN3CUlj",
    "outputId": "930e0b9b-e34d-4268-e594-5e5ec7c478e1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_encoded)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Predicted Model A', 'Predicted Model B'],\n",
    "            yticklabels=['Actual Model A', 'Actual Model B'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-H8pjmJ2B0N",
    "outputId": "20e1b52b-3f2e-443e-fe5d-923ae5775aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for each fold: [0.75007334 0.75359343 0.74655324 0.73364623 0.74185978]\n",
      "Mean CV Accuracy: 0.7451452038721034\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross-Validation\n",
    "\n",
    "X = task_a_df[features]\n",
    "y = task_a_df[\"winner\"]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(model_task_a, X, y, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(f'Accuracies for each fold: {cv_scores}')\n",
    "\n",
    "# Print the mean of the cross-validation accuracy\n",
    "print(f'Mean CV Accuracy: {cv_scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBn1QX1WCyxq"
   },
   "source": [
    "# **4. Task B**\n",
    "# Can we guess the hardness score of a given prompt?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YGVqLsxqRSs",
    "outputId": "9b5a839b-a057-4b22-c4c6-3d95a0642b71"
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Text Libraries\n",
    "from textstat import flesch_kincaid_grade, gunning_fog\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXv7w6lAXzC7",
    "outputId": "7514ae6e-088b-465c-f5c9-3078925ca76c"
   },
   "outputs": [],
   "source": [
    "embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "qH2-3gqVYlJ7",
    "outputId": "2b75e2d5-b82c-4900-8870-d96829e826ae"
   },
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings, columns=[f'e_{i}' for i in range(256)])\n",
    "embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NamvLlhSXGh0",
    "outputId": "4e8f887e-a3d7-49b8-d693-193389d220a1"
   },
   "outputs": [],
   "source": [
    "# Set up task B df\n",
    "\n",
    "task_b_df = pd.read_json(url_scores, lines=True)[['question_id', 'prompt', 'score_value_1', 'score_value_2', 'score_value_3']]\n",
    "# task_b_df = task_b_df.dropna(subset=['score_value_1', 'score_value_2', 'score_value_3'])\n",
    "# task_b_df['score_value_1'].astype(int)\n",
    "# task_b_df['average_score'] = task_b_df[['score_value_1', 'score_value_2','score_value_3']].mean(axis=1)\n",
    "\n",
    "# Add embeddings\n",
    "task_b_df.reset_index(drop=True, inplace=True)\n",
    "embeddings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "task_b_df = pd.concat([task_b_df, embeddings_df], axis=1)\n",
    "\n",
    "# Remove null values from scores values\n",
    "# topic_and_hardness = topic_and_hardness.dropna(subset=['score_value_1', 'score_value_2', 'score_value_3'])\n",
    "# print(topic_and_hardness[['score_value_1', 'score_value_2', 'score_value_3']].isna().sum())\n",
    "task_b_df = task_b_df.dropna(subset=['score_value_1', 'score_value_2', 'score_value_3'])\n",
    "print(task_b_df[['score_value_1', 'score_value_2', 'score_value_3']].isna().sum())\n",
    "\n",
    "# Change all score values to be numeric\n",
    "task_b_df.loc[:,'score_value_1'] = pd.to_numeric(task_b_df['score_value_1'], errors='coerce')\n",
    "task_b_df.loc[:,'score_value_2'] = pd.to_numeric(task_b_df['score_value_2'], errors='coerce')\n",
    "task_b_df.loc[:,'score_value_3'] = pd.to_numeric(task_b_df['score_value_3'], errors='coerce')\n",
    "\n",
    "# Drop any resulting null values\n",
    "task_b_df = task_b_df.dropna(subset=['score_value_1', 'score_value_2', 'score_value_3'])\n",
    "\n",
    "# Change all score values to be integers\n",
    "task_b_df['score_value_1'] = task_b_df['score_value_1'].astype(np.int64)\n",
    "task_b_df['score_value_2'] = task_b_df['score_value_2'].astype(np.int64)\n",
    "task_b_df['score_value_3'] = task_b_df['score_value_3'].astype(np.int64)\n",
    "\n",
    "# Verify all score values are integers\n",
    "task_b_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rmOvRrAz-i4",
    "outputId": "3e4e9636-5367-4399-c31d-9aa179a07162"
   },
   "outputs": [],
   "source": [
    "task_b_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "WNw9MCZJ3clx",
    "outputId": "5dc04595-1a8f-47b7-aaa2-764636571793"
   },
   "outputs": [],
   "source": [
    "# Calculate and add average score of each prompt to dataframe\n",
    "task_b_df['average_score'] = task_b_df[['score_value_1', 'score_value_2', 'score_value_3']].mean(axis=1)\n",
    "task_b_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "X3qFU8-s6e75",
    "outputId": "e0453584-bf8e-4b1c-cc14-6405af7f7611"
   },
   "outputs": [],
   "source": [
    "# Frequency of Hardness Scores\n",
    "\n",
    "plt.hist(task_b_df['average_score'])\n",
    "plt.title('Frequency of Hardness Score (Auto-Binned)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "PhsNM8wS6x-7",
    "outputId": "4bf14b99-a69e-4d87-a2bf-deeb53cdd958"
   },
   "outputs": [],
   "source": [
    "# Frequency of Hardness Scores Rounded Before Binning\n",
    "\n",
    "plt.hist(task_b_df['average_score'].round().astype(int))\n",
    "plt.title('Frequency of Hardness Score (Rounded Before Binning)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "J-bhW2GphjbG",
    "outputId": "70723ba0-3558-4975-d38a-8b694a5c5534"
   },
   "outputs": [],
   "source": [
    "task_b_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wiug86AegOD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICyUjk6EeguM"
   },
   "source": [
    "# 4a. Linear Regression Models (Feature Set 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Features include embeddings only*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1dCDNa2Zapp"
   },
   "source": [
    "**MODEL 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfrYyedAegSn",
    "outputId": "9a68c58b-5c3d-457b-82c7-5c42bace149f"
   },
   "outputs": [],
   "source": [
    "# Feature Set 1: Just embeddings\n",
    "\n",
    "# embeddings_matrix = task_b_df.drop(columns = ['question_id', 'cluster', 'score_value_1', 'score_value_2', 'score_value_3', 'average_score', 'total_hardness', 'prompt', 'cluster_score'])\n",
    "embeddings_matrix = task_b_df.drop(columns = ['question_id', 'score_value_1', 'score_value_2', 'score_value_3', 'average_score', 'prompt'])\n",
    "embeddings_matrix = embeddings_matrix.reset_index(drop=True)\n",
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0lECWAibded",
    "outputId": "47a40f6f-9260-4011-84ae-a65eb3e057c1"
   },
   "outputs": [],
   "source": [
    "# Feature Set 1 (Just embeddings)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_matrix, task_b_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model1a = LinearRegression()\n",
    "model1a.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions1a = model1a.predict(X_test).round().astype(int)\n",
    "mse1a = mean_squared_error(y_test, predictions1a)\n",
    "print(\"Mean Squared Error:\", mse1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va_XsK9Q7wFx",
    "outputId": "4437f6ce-16da-402a-90cc-20650100fda6"
   },
   "outputs": [],
   "source": [
    "# 5 K-Fold Cross-Validation\n",
    "\n",
    "X = embeddings_matrix\n",
    "y = task_b_df['average_score']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores1a = cross_val_score(model1a, X, y, cv=kf, scoring='neg_mean_squared_error')#.round().astype(int)\n",
    "\n",
    "# Since the scores are negative MSE, we might want to convert them to positive MSE\n",
    "mse_scores1a = -scores1a\n",
    "\n",
    "print(\"MSE scores for each fold:\", mse_scores1a)\n",
    "print(\"Average MSE:\", mse_scores1a.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZJ4APTtHt63"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUgAdNLdL_69"
   },
   "source": [
    "# 4b. Linear Regression Models (Feature Set 2)\n",
    "*Features include prompt quantifications, such as character count, word count, sentence count, words per sentence, unique words, and unique words to total words ratio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzWAHVQSHbgn"
   },
   "source": [
    "**MODEL 2A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0lvAsnvzBQE",
    "outputId": "8af7d529-899e-4981-8ceb-fe6c7914d6ec"
   },
   "outputs": [],
   "source": [
    "# Feature Set 2a **\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features2a(text):\n",
    "    char_count = len(text)\n",
    "    word_count = len(nltk.word_tokenize(text))\n",
    "    char_word_ratio = char_count/word_count\n",
    "    sent_count = len(nltk.sent_tokenize(text))\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    return [char_count, word_count, char_word_ratio, sent_count, words_per_sent, uniq_words]\n",
    "\n",
    "# Applying feature extraction\n",
    "features2a = merged_df['prompt'].apply(extract_features2a)\n",
    "features2a_df = pd.DataFrame(features2a.tolist(), columns=['char_count', 'word_count', 'char_word_ratio', 'sent_count', 'words_per_sent', 'uniq_words'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features2a_df, merged_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model2a = LinearRegression()\n",
    "model2a.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions2a = model2a.predict(X_test).round().astype(int)\n",
    "mse2a = mean_squared_error(y_test, predictions2a)\n",
    "print(\"Mean Squared Error:\", mse2a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-oTGVvi6C0h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICkNQ_EuF-cH"
   },
   "source": [
    "**MODEL 2B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJ_BQNg2zOoW",
    "outputId": "ed212a2a-0987-4439-c76e-b228bc222279"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features2b(text):\n",
    "    char_count = len(text)\n",
    "    word_count = len(nltk.word_tokenize(text))\n",
    "    char_word_ratio = char_count/word_count\n",
    "    sent_count = len(nltk.sent_tokenize(text))\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    fk_grade = textstat.flesch_kincaid_grade(text) # best\n",
    "    gf_index = textstat.gunning_fog(text)\n",
    "    return [char_count, word_count, char_word_ratio, sent_count, words_per_sent, uniq_words, uniq_word_ratio, fk_grade, gf_index]\n",
    "\n",
    "# Applying feature extraction\n",
    "features2b = merged_df['prompt'].apply(extract_features2b)\n",
    "features2b_df = pd.DataFrame(features2b.tolist(), columns=['char_count', 'word_count', 'char_word_ratio', 'sent_count', 'words_per_sent', 'uniq_words', 'uniq_word_ratio', 'fk_grade', 'gf_index'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features2b_df, merged_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model2b = LinearRegression()\n",
    "model2b.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions2b = model2b.predict(X_test).round().astype(int)\n",
    "mse2b = mean_squared_error(y_test, predictions2b)\n",
    "print(\"Mean Squared Error:\", mse2b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7UrUU096Dmc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGjFjRM0wqwm"
   },
   "source": [
    "**MODEL 2C**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvfR6F4gEhhO"
   },
   "source": [
    "Replace fk_grade and gf_index grades with aggregated grade\n",
    "\n",
    "* char_count = textstat.char_count(text)\n",
    "* word_count = textstat.lexicon_count(text, removepunct=True)\n",
    "* sent_count = textstat.sentence_count(text)\n",
    "* words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "* polysyll_count = textstat.polysyllabcount(text)\n",
    "* uniq_words = len(set(word_tokenize(text)))\n",
    "* uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "* grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9s3hvxSzfGt",
    "outputId": "0c3b89c2-76cf-48e3-8709-10edad8bfb1d"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features2c(text):\n",
    "    char_count = textstat.char_count(text)\n",
    "    word_count = textstat.lexicon_count(text, removepunct=True)\n",
    "    sent_count = textstat.sentence_count(text)\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    polysyll_count = textstat.polysyllabcount(text)\n",
    "    uniq_words = len(set(word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    grade = textstat.text_standard(text, float_output=True)\n",
    "    return [char_count, word_count, sent_count, words_per_sent, polysyll_count, uniq_words, uniq_word_ratio, grade]#, readability, grade]\n",
    "\n",
    "# Applying feature extraction\n",
    "features2c = merged_df['prompt'].apply(extract_features2c)\n",
    "features2c_df = pd.DataFrame(features2c.tolist(),\n",
    "                            columns=['char_count', 'word_count', 'sent_count', 'words_per_sent',\n",
    "                                     'polysyll_count', 'uniq_words', 'uniq_word_ratio',\n",
    "                                     'complexity'])#, 'readability', 'grade'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features2c_df, merged_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model2c = LinearRegression()\n",
    "model2c.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions2c = model2c.predict(X_test).round().astype(int)\n",
    "mse2c = mean_squared_error(y_test, predictions2c)\n",
    "print(\"Mean Squared Error:\", mse2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9TLV0nW6FgE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD8EFFi9r1IS"
   },
   "source": [
    "**MODEL 2D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOJF9jYhEkVc"
   },
   "source": [
    "Swap aggregated grade score with just Flesh-Kincaid and Gunning Fog grade-scoring indexes.\n",
    "\n",
    "* char_count\n",
    "* word_count\n",
    "* sent_count\n",
    "* words_per_sent\n",
    "* polysyll_count\n",
    "* uniq_words\n",
    "* uniq_word_ratio\n",
    "* fk_grade\n",
    "* gf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5qC69lcz6-X",
    "outputId": "ac99b8ec-d431-4979-ec23-087987d4ab45"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features2d(text):\n",
    "    char_count = textstat.char_count(text)\n",
    "    word_count = textstat.lexicon_count(text, removepunct=True)\n",
    "    sent_count = textstat.sentence_count(text)\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    polysyll_count = textstat.polysyllabcount(text)\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    fk_grade = textstat.flesch_kincaid_grade(text)\n",
    "    gf_index = textstat.gunning_fog(text)\n",
    "    return [char_count, word_count, sent_count, words_per_sent, polysyll_count, uniq_words, uniq_word_ratio, fk_grade, gf_index]\n",
    "\n",
    "# Applying feature extraction\n",
    "features2d = merged_df['prompt'].apply(extract_features2d)\n",
    "features2d_df = pd.DataFrame(features2d.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent', 'polysyll_count', 'uniq_words', 'uniq_word_ratio', 'fk_grade', 'gf_index'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features2d_df, merged_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model2d = LinearRegression()\n",
    "model2d.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions2d = model2d.predict(X_test).round().astype(int)\n",
    "mse2d = mean_squared_error(y_test, predictions2d)\n",
    "print(\"Mean Squared Error:\", mse2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_yD4oZa0H31",
    "outputId": "b8312352-233c-4a88-a7e7-a59658be5d20"
   },
   "outputs": [],
   "source": [
    "# 5 K-Fold Cross-Validation\n",
    "\n",
    "X = features2d_df\n",
    "y = merged_df['average_score']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores2d = cross_val_score(model2d, X, y, cv=kf, scoring='neg_mean_squared_error')#.round().astype(int)\n",
    "\n",
    "# Since the scores are negative MSE, we might want to convert them to positive MSE\n",
    "mse_scores2d = -scores2d\n",
    "\n",
    "print(\"MSE scores for each fold:\", mse_scores2d)\n",
    "print(\"Average MSE:\", mse_scores2d.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzTmyb6z6HJ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An8QOWZ2KJL0"
   },
   "source": [
    "**MODEL 2E**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClD5wOJuEonS"
   },
   "source": [
    "Added in read time and complexity (ease score).\n",
    "\n",
    "* char_count\n",
    "* word_count\n",
    "* sent_count\n",
    "* words_per_sent\n",
    "* polysyll_count\n",
    "* uniq_words\n",
    "* uniq_word_ratio\n",
    "* fk_grade\n",
    "* gf_index\n",
    "* read_time\n",
    "* complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rwoxv1zC0jBJ",
    "outputId": "385e530e-7bbb-42d1-db8c-e6f8f9e1d620"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features2e(text):\n",
    "    char_count = textstat.char_count(text)\n",
    "    word_count = textstat.lexicon_count(text, removepunct=True)\n",
    "    sent_count = textstat.sentence_count(text)\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    polysyll_count = textstat.polysyllabcount(text)\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    fk_grade = textstat.flesch_kincaid_grade(text)\n",
    "    gf_index = textstat.gunning_fog(text)\n",
    "    read_time = textstat.reading_time(text, ms_per_char=14.69)\n",
    "    complexity = textstat.flesch_reading_ease(text)\n",
    "    return [char_count, word_count, sent_count, words_per_sent, polysyll_count, uniq_words, uniq_word_ratio, fk_grade, gf_index, read_time, complexity]\n",
    "\n",
    "# Applying feature extraction\n",
    "features2e = merged_df['prompt'].apply(extract_features2e)\n",
    "features2e_df = pd.DataFrame(features2e.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent',\n",
    "                                                         'polysyll_count', 'uniq_words', 'uniq_word_ratio',\n",
    "                                                         'fk_grade', 'gf_index', 'readability', 'complexity'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features2e_df, merged_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model2e = LinearRegression()\n",
    "model2e.fit(X_train, y_train)\n",
    "\n",
    "predictions2e = model2e.predict(X_test).round().astype(int)\n",
    "mse2e = mean_squared_error(y_test, predictions2e)\n",
    "print(\"Mean Squared Error:\", mse2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyNrrwqkNvH-",
    "outputId": "6a68f890-6ac9-468e-a7db-bf499db681d6"
   },
   "outputs": [],
   "source": [
    "# 5 K-Fold Cross-Validation\n",
    "\n",
    "X = features2e_df\n",
    "y = merged_df['average_score']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores2e = cross_val_score(model2e, X, y, cv=kf, scoring='neg_mean_squared_error')#.round().astype(int)\n",
    "\n",
    "# Since the scores are negative MSE, we might want to convert them to positive MSE\n",
    "mse_scores2e = -scores2e\n",
    "\n",
    "print(\"MSE scores for each fold:\", mse_scores2e)\n",
    "print(\"Average MSE:\", mse_scores2e.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5dWTKCFkzkB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9ur1uSekmw2"
   },
   "source": [
    "## 4c. Linear Regression Models (Feature Set 3)\n",
    "*Features include combinations of prompt quantifications and embeddings*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9jvznrU6MUa"
   },
   "source": [
    "**MODEL 3A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcAXRwlAw9Zh",
    "outputId": "d50f9a70-da8d-4ccf-d58f-96ed4fff4721"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features3a(text):\n",
    "    char_count = len(text)\n",
    "    word_count = len(nltk.word_tokenize(text))\n",
    "    char_word_ratio = char_count/word_count\n",
    "    sent_count = len(nltk.sent_tokenize(text))\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    return [char_count, word_count, char_word_ratio, sent_count, words_per_sent, uniq_words]\n",
    "\n",
    "# Applying feature extraction\n",
    "features3a = task_b_df['prompt'].apply(extract_features3a)\n",
    "features3a_df = pd.DataFrame(features3a.tolist(), columns=['char_count', 'word_count', 'char_word_ratio', 'sent_count', 'words_per_sent', 'uniq_words'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "features3a_df = features3a_df.reset_index(drop=True)\n",
    "\n",
    "# Add embeddings to features\n",
    "features3a_df = pd.concat([features3a_df, embeddings_matrix], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features3a_df, task_b_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Model training\n",
    "model3a = LinearRegression()\n",
    "model3a.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions3a = model3a.predict(X_test).round().astype(int)\n",
    "mse3a = mean_squared_error(y_test, predictions3a)\n",
    "print(\"Mean Squared Error:\", mse3a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqASXt5ucrsO",
    "outputId": "9f05448f-30a3-48e5-dc26-fa114f1a9186"
   },
   "outputs": [],
   "source": [
    "# NEW FEATURE SET 3a (prompt-text + embeddings)\n",
    "\n",
    "# 5 K-Fold Cross-Validation\n",
    "\n",
    "X = features3a_df\n",
    "y = task_b_df['average_score']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores3a = cross_val_score(model3a, X, y, cv=kf, scoring='neg_mean_squared_error')#.round().astype(int)\n",
    "\n",
    "# Since the scores are negative MSE, we might want to convert them to positive MSE\n",
    "mse3a_scores = -scores3a\n",
    "\n",
    "print(\"MSE scores for each fold:\", mse3a_scores)\n",
    "print(\"Average MSE:\", mse3a_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXJgYrKM6TBb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esb1PlYI6TqN"
   },
   "source": [
    "**MODEL 3B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeiBgNU1hnIG",
    "outputId": "36fb571d-b7cc-469d-9b37-5c5fe5c45a2e"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features3b(text):\n",
    "    char_count = textstat.char_count(text)\n",
    "    word_count = textstat.lexicon_count(text, removepunct=True)\n",
    "    sent_count = textstat.sentence_count(text) # not used in actual feature set\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    polysyll_count = textstat.polysyllabcount(text)\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    fk_grade = textstat.flesch_kincaid_grade(text)\n",
    "    gf_index = textstat.gunning_fog(text)\n",
    "    return [char_count, word_count, sent_count, words_per_sent, polysyll_count, uniq_words, uniq_word_ratio, fk_grade, gf_index]\n",
    "\n",
    "# Applying feature extraction\n",
    "features3b = task_b_df['prompt'].apply(extract_features3b)\n",
    "features3b_df = pd.DataFrame(features3b.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent', 'polysyll_count', 'uniq_words', 'uniq_word_ratio', 'fk_grade', 'gf_index'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "features3b_df = features3b_df.reset_index(drop=True)\n",
    "# Add embeddings\n",
    "features3b_df = pd.concat([features3b_df, embeddings_matrix], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features3b_df, task_b_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model3b = LinearRegression()\n",
    "model3b.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions3b = model3b.predict(X_test).round().astype(int)\n",
    "mse3b = mean_squared_error(y_test, predictions3b)\n",
    "print(\"Mean Squared Error:\", mse3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yuopk5tKhgZP",
    "outputId": "23d277bd-da79-4aab-f9b5-be63cb3fb471"
   },
   "outputs": [],
   "source": [
    "# Feature Set 3b (prompt-text + complexity + embeddings)\n",
    "\n",
    "# 5 K-Fold Cross-Validation\n",
    "\n",
    "X = features3b_df\n",
    "y = task_b_df['average_score']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores3b = cross_val_score(model3b, X, y, cv=kf, scoring='neg_mean_squared_error')#.round().astype(int)\n",
    "\n",
    "# Since the scores are negative MSE, we might want to convert them to positive MSE\n",
    "mse3b_scores = -scores3b\n",
    "\n",
    "print(\"MSE scores for each fold:\", mse3b_scores)\n",
    "print(\"Average MSE:\", mse3b_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qUwjUIP6YCp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_M_DtWr6YRk"
   },
   "source": [
    "**MODEL 3C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3613R3Si7IN",
    "outputId": "00a92b62-8b4c-43b1-ebb6-1c31d5b086b9"
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features3c(text):\n",
    "    char_count = textstat.char_count(text)\n",
    "    word_count = textstat.lexicon_count(text, removepunct=True)\n",
    "    sent_count = textstat.sentence_count(text)\n",
    "    words_per_sent = word_count / sent_count if sent_count > 0 else 0\n",
    "    polysyll_count = textstat.polysyllabcount(text)\n",
    "    uniq_words = len(set(nltk.word_tokenize(text)))\n",
    "    uniq_word_ratio = uniq_words / word_count if word_count > 0 else 0\n",
    "    fk_grade = textstat.flesch_kincaid_grade(text)\n",
    "    gf_index = textstat.gunning_fog(text)\n",
    "    read_time = textstat.reading_time(text, ms_per_char=14.69)\n",
    "    complexity = textstat.flesch_reading_ease(text)\n",
    "    return [char_count, word_count, sent_count, words_per_sent, polysyll_count, uniq_words, uniq_word_ratio, fk_grade, gf_index, read_time, complexity]\n",
    "\n",
    "# Applying feature extraction\n",
    "features3c = task_b_df['prompt'].apply(extract_features3c)\n",
    "features3c_df = pd.DataFrame(features3c.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent',\n",
    "                                                         'polysyll_count', 'uniq_words', 'uniq_word_ratio',\n",
    "                                                         'fk_grade', 'gf_index', 'readability', 'complexity'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "features3c_df = features3c_df.reset_index(drop=True)\n",
    "\n",
    "# Add embeddings to feature set\n",
    "features3c_df = pd.concat([features3c_df, embeddings_matrix], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features3c_df, task_b_df['average_score'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model3c = LinearRegression()\n",
    "model3c.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions3c = model3c.predict(X_test).round().astype(int)\n",
    "mse3c = mean_squared_error(y_test, predictions3c)\n",
    "print(\"Mean Squared Error:\", mse3c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN2HAhjjjHez",
    "outputId": "2da5497f-5d7e-4b27-d7e5-d2feba9d9f46"
   },
   "outputs": [],
   "source": [
    "# Feature Set 3c (prompt-text + complexity + embeddings)\n",
    "\n",
    "# 5 K-Fold Cross-Validation\n",
    "\n",
    "X = features3c_df\n",
    "y = task_b_df['average_score']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores3c = cross_val_score(model3c, X, y, cv=kf, scoring='neg_mean_squared_error')#.round().astype(int)\n",
    "\n",
    "# Since the scores are negative MSE, we might want to convert them to positive MSE\n",
    "mse3c_scores = -scores3c\n",
    "\n",
    "print(\"MSE scores for each fold:\", mse3c_scores)\n",
    "print(\"Average MSE:\", mse3c_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv0HpDUUk2bl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZjwY6ztJM_2"
   },
   "source": [
    "# **5. Test on Validation Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkdaZtA2WZfD"
   },
   "source": [
    "## 5a. Import & Clean Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "UZ2h7VhYJdzb",
    "outputId": "9885ea38-8900-4001-c473-18aa720100af"
   },
   "outputs": [],
   "source": [
    "# Import validation set - prompts\n",
    "\n",
    "# Provide the raw URL of the JSON file\n",
    "validation_prompts_url = \"https://raw.githubusercontent.com/dychenster/nlp-chatarena/main/arena-validation-set-prompt-only.jsonl.gz\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "validation_prompts = pd.read_json(validation_prompts_url, lines=True)\n",
    "\n",
    "# Display the first row of the data\n",
    "display(validation_prompts.head(1))\n",
    "display(validation_prompts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "gwqSyhT9KZ9q",
    "outputId": "c96d4b97-0be2-42e4-c83c-7c35ebb8e0c0"
   },
   "outputs": [],
   "source": [
    "# Import validation set - topic modeling\n",
    "\n",
    "# Provide the raw URL of the JSON file\n",
    "validation_topics_url = \"https://raw.githubusercontent.com/dychenster/nlp-chatarena/main/arena-validation-set-topic-modeling.jsonl.gz\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "validation_topics = pd.read_json(validation_topics_url, lines=True)\n",
    "\n",
    "# Display the first row of the data\n",
    "display(validation_topics.head(1))\n",
    "display(validation_topics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8MKrPhKAPLX",
    "outputId": "a7612ae3-f4c6-4457-d505-c63b84a05bbc"
   },
   "outputs": [],
   "source": [
    "# Import validation set - embeddings\n",
    "\n",
    "# Provide the raw URL of the JSON file\n",
    "validation_embeddings_url = \"https://raw.githubusercontent.com/dychenster/nlp-chatarena/main/arena-validation-set-prompts-embeddings.npy\"\n",
    "\n",
    "# Use requests to get the file content\n",
    "response_val = requests.get(validation_embeddings_url)\n",
    "\n",
    "# Make sure the request was successful\n",
    "if response_val.status_code == 200:\n",
    "    # Load the content into a numpy array\n",
    "    validation_content = BytesIO(response_val.content)\n",
    "    validation_embeddings = np.load(validation_content)\n",
    "    print(\"Embeddings loaded successfully into Colab.\")\n",
    "else:\n",
    "    print(f\"Failed to load the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tP5uMewBLFQ",
    "outputId": "0c0cee08-72e6-4294-8286-42dbbab8c376"
   },
   "outputs": [],
   "source": [
    "val_embeddings_df = pd.DataFrame(validation_embeddings, columns=[f'e_{i}' for i in range(256)])\n",
    "val_embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-qBylfWLj33",
    "outputId": "df249e73-3ec9-40c8-e8b5-09470a955be8"
   },
   "outputs": [],
   "source": [
    "validation_df = validation_prompts.merge(validation_topics, on=\"question_id\")\n",
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR3NJy6oL-03",
    "outputId": "9a6d82e5-ce39-4483-86d0-ad7f2e07085b"
   },
   "outputs": [],
   "source": [
    "print(validation_prompts['question_id'].duplicated().sum())\n",
    "print(validation_topics['question_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEntN1Z1NI36",
    "outputId": "57482933-db0d-4db5-fce1-0e6263c6f93a"
   },
   "outputs": [],
   "source": [
    "# Identify duplicate rows in the 'validation_prompts' DataFrame\n",
    "duplicate_prompts = validation_prompts[validation_prompts.duplicated(subset='question_id', keep=False)]\n",
    "\n",
    "# Identify duplicate rows in the 'validation_topics' DataFrame\n",
    "duplicate_topics = validation_topics[validation_topics.duplicated(subset='question_id', keep=False)]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate rows in validation_prompts based on question_id:\")\n",
    "print(duplicate_prompts)\n",
    "\n",
    "print(\"\\nDuplicate rows in validation_topics based on question_id:\")\n",
    "print(duplicate_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cS978HZyU_UV",
    "outputId": "2fc4bbe8-9189-4b8b-d4cd-e52cb9a27818"
   },
   "outputs": [],
   "source": [
    "# Add a column to mark the first two duplicates\n",
    "validation_df['duplicate_count'] = validation_df.groupby('question_id').cumcount()\n",
    "\n",
    "# Keep only the first two duplicates (i.e., where duplicate_count is 0 or 1)\n",
    "validation_df_filtered = validation_df[validation_df['duplicate_count'] < 2].copy()\n",
    "\n",
    "# Now drop the 'duplicate_count' column as it's no longer needed\n",
    "validation_df_filtered.drop('duplicate_count', axis=1, inplace=True)\n",
    "\n",
    "# Verify the result\n",
    "print(validation_df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "sdFe12ChWeb1",
    "outputId": "0dc60950-08f7-4889-8284-0d11688032ea"
   },
   "outputs": [],
   "source": [
    "validation_df_filtered.drop(columns=[\"prompt_y\"], inplace=True)\n",
    "validation_df_filtered.rename(columns = {\"prompt_x\":\"prompt\"}, inplace=True)\n",
    "validation_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "32xqz-LEH90-",
    "outputId": "0c99f6d1-a0fe-4089-f784-2ce1ab0ccfd6"
   },
   "outputs": [],
   "source": [
    "validation_df_filtered.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "yNqNfPJWInAw",
    "outputId": "1c0ac30e-be15-496f-93aa-5c46c731a674"
   },
   "outputs": [],
   "source": [
    "task_b_val_df = validation_df_filtered.copy()\n",
    "\n",
    "# Add embeddings\n",
    "task_b_val_df.reset_index(drop=True, inplace=True)\n",
    "val_embeddings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "task_b_val_df = pd.concat([task_b_val_df, val_embeddings_df], axis=1)\n",
    "task_b_val_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdX-2GRj8n2T"
   },
   "source": [
    "## 5b. Test Models Using Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71HGhr-i3c7l"
   },
   "outputs": [],
   "source": [
    "# Validation Distribution for Reference\n",
    "\n",
    "# hardness_score\n",
    "# 1     204\n",
    "# 2     377\n",
    "# 3      87\n",
    "# 4      74\n",
    "# 5     189\n",
    "# 6     182\n",
    "# 7    1087\n",
    "# 8     913\n",
    "# 9      93\n",
    "# Name: count, dtype: int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtMYPwBMWAX1"
   },
   "source": [
    "**TASK A PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VtnSde8pr5U"
   },
   "outputs": [],
   "source": [
    "# Create dataframe for Task A testing\n",
    "\n",
    "task_a_df = validation_df_filtered.copy()\n",
    "\n",
    "task_a_df[\"prompt_length\"] = task_a_df[\"prompt\"].str.len()\n",
    "task_a_df[\"prompt_complexity\"] = (task_a_df[\"prompt\"]).apply(textstat.flesch_reading_ease)\n",
    "task_a_df[\"prompt_grade\"] = task_a_df[\"prompt\"].apply(lambda x: textstat.text_standard(x, float_output=True))\n",
    "task_a_df[\"prompt_time\"] = task_a_df[\"prompt\"].apply(lambda x: textstat.reading_time(x, ms_per_char=14.69))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MMhpmVg8_RG"
   },
   "outputs": [],
   "source": [
    "task_a_df[\"model_a_win_rate\"] = task_a_df[\"model_a\"].map(win_rates)\n",
    "task_a_df[\"model_b_win_rate\"] = task_a_df[\"model_b\"].map(win_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nqbfYHzRlr8",
    "outputId": "16309fee-9645-4939-fec4-eca141f983ab"
   },
   "outputs": [],
   "source": [
    "models_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vygWpMg9at7"
   },
   "outputs": [],
   "source": [
    "# Initialize the columns to zero\n",
    "for model in models_combined:\n",
    "    task_a_df[model] = 0\n",
    "\n",
    "# Use logical OR to combine the one-hot encoding for both model_a and model_b\n",
    "for model in models_combined:\n",
    "    task_a_df[model] = ((task_a_df[\"model_a\"] == model) | (task_a_df[\"model_b\"] == model)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qk9VcWMR6DC",
    "outputId": "adc3ef5e-10db-4933-84ce-ee961a2492e2"
   },
   "outputs": [],
   "source": [
    "task_a_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX7qJO47WFIP"
   },
   "outputs": [],
   "source": [
    "X_validation = task_a_df[features]\n",
    "y_validation_pred_encoded = model_task_a.predict(X_validation)\n",
    "\n",
    "# Decode the predictions back to the original labels\n",
    "winner_predictions = le.inverse_transform(y_validation_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6se8ARF439e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0reEzFqAE1ur"
   },
   "source": [
    "**TASK B PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine variability in model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4MtChVW-2Hx",
    "outputId": "6ab48367-57b0-4adb-cfe9-dc637cc390b1"
   },
   "outputs": [],
   "source": [
    "# TEST MODEL 1A\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions1a = model1a.predict(val_embeddings_df).astype(int)\n",
    "print(\"Predicted Hardness Score:\", val_predictions1a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYHBvI0zPaeR",
    "outputId": "b6841395-20c8-4783-9027-119426b1ea76"
   },
   "outputs": [],
   "source": [
    "# TEST MODEL 2D\n",
    "\n",
    "# Applying feature extraction\n",
    "val_features2d = task_b_val_df['prompt'].apply(extract_features2d)\n",
    "val_features2d_df = pd.DataFrame(val_features2d.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent', 'polysyll_count', 'uniq_words', 'uniq_word_ratio', 'fk_grade', 'gf_index'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "val_features2d_df = val_features2d_df.reset_index(drop=True)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions2d = model2d.predict(val_features2d_df).round().astype(int)\n",
    "print(\"Predicted Hardness Score:\", val_predictions2d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZSHuO_3IzsJ",
    "outputId": "f2227387-5e82-4b4e-aa7c-e222cc1d0c0e"
   },
   "outputs": [],
   "source": [
    "# TEST MODEL 2E\n",
    "\n",
    "# Applying feature extraction\n",
    "val_features2e = task_b_val_df['prompt'].apply(extract_features2e)\n",
    "val_features2e_df = pd.DataFrame(val_features2e.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent',\n",
    "                                                         'polysyll_count', 'uniq_words', 'uniq_word_ratio',\n",
    "                                                         'fk_grade', 'gf_index', 'readability', 'complexity'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "val_features2e_df = val_features2e_df.reset_index(drop=True)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions2e = model2e.predict(val_features2e_df).round().astype(int)\n",
    "print(\"Predicted Hardness Score:\", val_predictions2e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Zn-uF6qE0lJ",
    "outputId": "90aad155-9c94-4be3-8c9d-3e1d01a142f0"
   },
   "outputs": [],
   "source": [
    "# TEST MODEL 3A\n",
    "\n",
    "# Applying feature extraction\n",
    "val_features3a = task_b_val_df['prompt'].apply(extract_features3a)\n",
    "val_features3a_df = pd.DataFrame(val_features3a.tolist(), columns=['char_count', 'word_count', 'char_word_ratio', 'sent_count', 'words_per_sent', 'uniq_words'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "val_features3a_df = val_features3a_df.reset_index(drop=True)\n",
    "\n",
    "val_features3a_df = pd.concat([val_features3a_df, val_embeddings_df], axis=1)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions3a = model3a.predict(val_features3a_df).astype(int)\n",
    "print(\"Predicted Hardness Score:\", val_predictions3a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Th9VkmTewxhK",
    "outputId": "623bfd1f-7570-4951-d699-24386de8fe49"
   },
   "outputs": [],
   "source": [
    "# TEST MODEL 3C\n",
    "\n",
    "# Applying feature extraction\n",
    "val_features3c = task_b_val_df['prompt'].apply(extract_features3c)\n",
    "val_features3c_df = pd.DataFrame(val_features3c.tolist(), columns=['char_count', 'word_count', 'sent_count', 'words_per_sent',\n",
    "                                                         'polysyll_count', 'uniq_words', 'uniq_word_ratio',\n",
    "                                                         'fk_grade', 'gf_index', 'readability', 'complexity'])\n",
    "\n",
    "# Reset the indices of both DataFrames if the indices do not matter\n",
    "val_features3c_df = val_features3c_df.reset_index(drop=True)\n",
    "# Add embeddings\n",
    "val_features3c_df = pd.concat([val_features3c_df, val_embeddings_df], axis=1)\n",
    "\n",
    "# Predict and evaluate\n",
    "val_predictions3c = model3c.predict(val_features3c_df).round().astype(int)\n",
    "print(\"Predicted Hardness Score:\", val_predictions3c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp1y-_RP_WBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq4sonQ5QlSO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LCH7SwVpsib"
   },
   "source": [
    "# **6. Create CSV for Download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1A PREDICTIONS\n",
    "\n",
    "# Create DataFrame with specified columns\n",
    "submission1a_df = pd.DataFrame({\n",
    "    'question_id': validation_df_filtered['question_id'],\n",
    "    'winner': winner_predictions, # list or array of winners\n",
    "    'hardness_score': val_predictions1a},\n",
    "    columns=['question_id', 'winner', 'hardness_score'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission1a_df.to_csv('submission1a.csv', index=False)\n",
    "\n",
    "# Provide a link to download the file\n",
    "display(FileLink('submission1a.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2D PREDICTIONS\n",
    "\n",
    "# Create DataFrame with specified columns\n",
    "submission2d_df = pd.DataFrame({\n",
    "    'question_id': validation_df_filtered['question_id'],\n",
    "    'winner': winner_predictions, # list or array of winners\n",
    "    'hardness_score': val_predictions2d},\n",
    "    columns=['question_id', 'winner', 'hardness_score'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission2d_df.to_csv('submission2d.csv', index=False)\n",
    "\n",
    "# Provide a link to download the file\n",
    "display(FileLink('submission2d.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2E PREDICTIONS\n",
    "\n",
    "# Create DataFrame with specified columns\n",
    "submission2e_df = pd.DataFrame({\n",
    "    'question_id': validation_df_filtered['question_id'],\n",
    "    'winner': winner_predictions, # list or array of winners\n",
    "    'hardness_score': val_predictions2e},\n",
    "    columns=['question_id', 'winner', 'hardness_score'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission2e_df.to_csv('submission2e.csv', index=False)\n",
    "\n",
    "# Provide a link to download the file\n",
    "display(FileLink('submission2e.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkmF9yDDQ5y0"
   },
   "outputs": [],
   "source": [
    "# MODEL 3A PREDICTIONS\n",
    "\n",
    "# Create DataFrame with specified columns\n",
    "submission3a_df = pd.DataFrame({\n",
    "    'question_id': validation_df_filtered['question_id'],\n",
    "    'winner': winner_predictions, # list or array of winners\n",
    "    'hardness_score': val_predictions3a},\n",
    "    columns=['question_id', 'winner', 'hardness_score'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission3a_df.to_csv('submission3a.csv', index=False)\n",
    "\n",
    "# Provide a link to download the file\n",
    "display(FileLink('submission3a.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "k3OsthlhkMek",
    "outputId": "9d3677b9-65b6-4c49-dea9-be186a007620"
   },
   "outputs": [],
   "source": [
    "# MODEL 3C PREDICTIONS\n",
    "\n",
    "# Create DataFrame with specified columns\n",
    "submission3c_df = pd.DataFrame({\n",
    "    'question_id': validation_df_filtered['question_id'],\n",
    "    'winner': winner_predictions, # list or array of winners\n",
    "    'hardness_score': val_predictions3c},\n",
    "    columns=['question_id', 'winner', 'hardness_score'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission3c_df.to_csv('submission3c.csv', index=False)\n",
    "\n",
    "# Provide a link to download the file\n",
    "display(FileLink('submission3c.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0M3J2hmHoO9J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17e_GGrM4Zv6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***References:***\n",
    "\n",
    "[1] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \"A Survey on Bias and Fairness in Machine Learning,\" arXiv:1908.09635v2 [cs.LG], Sep. 2019. Available: https://arxiv.org/abs/1908.09635\n",
    "\n",
    "[2] C.-H. Chiang and H.-y. Lee, \"Can Large Language Models Be an Alternative to Human Evaluation?\" in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), Volume 1: Long Papers, pp. 15607–15631, July 2023. Available: https://www.aclweb.org/anthology/2023.acl-long.870\n",
    "\n",
    "[3] van der Lee, C., Gatt, A., van Miltenburg, E., & Krahmer, E. (2021). Human evaluation of automatically generated text: Current trends and best practice guidelines. Computer Speech &amp; Language, 67, 101151. doi:10.1016/j.csl.2020.101151\n",
    "\n",
    "[4] W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, B. Zhu, H. Zhang, M. I. Jordan, J. E. Gonzalez, and I. Stoica, \"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,\" arXiv:2403.04132v1 [cs.AI], Mar. 2024. Available: https://arxiv.org/abs/2403.04132\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
